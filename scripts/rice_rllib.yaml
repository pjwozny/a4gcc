# Copyright (c) 2022, salesforce.com, inc and MILA.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root
# or https://opensource.org/licenses/BSD-3-Clause


# Checkpoint saving setting
saving:
    metrics_log_freq: 100 # how often (in episodes) to log (and print) the metrics
    model_params_save_freq: 100 # how often (in episodes) to save the model parameters
    basedir: "/tmp" # base folder used for saving
    name: "rice" # experiment name
    tag: "experiments" # experiment tag

# Trainer settings
trainer:
    num_workers: 30 # number of rollout worker actors to create for parallel sampling.
    # Note: Setting the num_workers to 0 will force rollouts to be done in the trainer actor.
    num_envs_per_worker: 1 # number of environment replicas per worker
    batch_mode: "complete_episodes" # complete_episodes/truncate_episodes
    train_batch_size_in_episodes: 60 # total batch size used for training per iteration (across all the environments)
    num_episodes: 100000 # number of episodes to run the training for
    framework: torch # framework setting.
    # Note: RLlib supports TF as well, but our end-to-end pipeline is built for Pytorch only.
    ignore_worker_failures: True
    recreate_failed_workers: True
    # === Hardware Settings ===
    num_cpus_per_worker: 1
    num_gpus: 1 # number of GPUs to allocate to the trainer process. This can also be fractional (e.g., 0.3 GPUs).

logging:
    enabled: True #default logging to false so that
    wandb_config:
        # login: "91f4b56e70eb59889967350b045b94cd0d7bcaa8" # Philip
        login: "d99374501385e0e6bb12a92f9e291405acd2c145" # Bram
        project: "test"
        run: "test"
        entity: "ai4gcc"

# Environment configuration
env:
    num_discrete_action_levels: 10 # number of discrete levels for the saving and mitigation actions
    negotiation_on: True # flag to indicate whether negotiation is allowed or not
    negotiator_class_config:
        file_name: "negotiator"
        class_name: "BilateralNegotiatorWithOnlyTariff"

# Policy network settings
policy:
    regions:
        vf_loss_coeff: 0.1 # loss coefficient schedule for the value function loss
        entropy_coeff_schedule: # loss coefficient schedule for the entropy loss
        # piecewise linear, specified as (timestep, value) (timestep is seen per agent)
            - [0, 0.5]
            - [800000, 0.1]
            - [1400000, 0.05]
        clip_grad_norm: True # flag indicating whether to clip the gradient norm or not
        max_grad_norm: 0.5 # when clip_grad_norm is True, the clip level
        gamma: 0.92 # discount factor
        lr: 0.0005 # learning rate
        model:
            custom_model: torch_linear
            custom_model_config:
                fc_dims: [256, 256]
